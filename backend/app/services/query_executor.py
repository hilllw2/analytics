"""
Query Executor Service
Safely executes pandas code generated by LLM.
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, Tuple, List
import traceback
from io import StringIO
import sys
from contextlib import redirect_stdout, redirect_stderr
import ast
from datetime import datetime, date

from app.core.session_manager import Session
from app.core.config import settings


def make_serializable(obj: Any) -> Any:
    """Convert to JSON-serializable format; preserve inf/NaN as strings so data is not lost."""
    if obj is None:
        return None
    if isinstance(obj, (str, int, bool)):
        return obj
    if isinstance(obj, (float, np.floating)):
        if np.isnan(obj):
            return "NaN"
        if np.isposinf(obj):
            return "Infinity"
        if np.isneginf(obj):
            return "-Infinity"
        return float(obj)
    if isinstance(obj, (datetime, date, pd.Timestamp)):
        return obj.isoformat() if pd.notna(obj) else None
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, np.bool_):
        return bool(obj)
    if isinstance(obj, (np.ndarray,)):
        return [make_serializable(x) for x in obj.tolist()]
    if isinstance(obj, pd.Period):
        return str(obj)
    if pd.isna(obj):
        return "NaN"
    return str(obj)


def serialize_df_to_records(df: pd.DataFrame) -> List[dict]:
    """Convert DataFrame to list of dicts with proper serialization."""
    records = []
    for _, row in df.iterrows():
        record = {}
        for col in df.columns:
            record[str(col)] = make_serializable(row[col])
        records.append(record)
    return records


def _safe_import(name: str, globals=None, locals=None, fromlist=(), level=0):
    """Restricted __import__ that only allows whitelisted modules. Used so LLM-generated code can use 'import pandas' etc."""
    # Normalize: "pandas as pd" is from importlib; we only get first part here
    base = name.split('.')[0].strip()
    if base not in ('pandas', 'numpy', 'datetime', 'math', 'scipy'):
        raise ImportError(f"Import of '{name}' is not allowed. Use the pre-injected 'pd', 'np', and 'df' instead.")
    return __import__(name, globals, locals, fromlist, level)


class SafeExecutor:
    """
    Safely executes pandas code with restricted globals.
    """
    
    # Allowed modules and functions (include __import__ so "import pandas" in generated code works)
    # Also include getattr/setattr/type/repr and common exceptions so LLM-generated code doesn't hit NameError
    SAFE_BUILTINS = {
        'abs', 'all', 'any', 'bool', 'dict', 'enumerate', 'filter',
        'float', 'format', 'frozenset', 'int', 'isinstance', 'len',
        'list', 'map', 'max', 'min', 'pow', 'print', 'range', 'reversed',
        'round', 'set', 'slice', 'sorted', 'str', 'sum', 'tuple', 'zip',
        'getattr', 'setattr', 'hasattr', 'type', 'repr', 'ord', 'chr',
        'iter', 'next', 'object', 'property', 'staticmethod',
        'Exception', 'BaseException', 'ValueError', 'KeyError', 'IndexError',
        'TypeError', 'AttributeError', 'ZeroDivisionError', 'RuntimeError',
        'StopIteration', 'AssertionError', 'ImportError',
        'True', 'False', 'None', '__import__'
    }
    
    # Dangerous patterns to block (do not block __import__ - we provide safe one)
    BLOCKED_PATTERNS = [
        'import os', 'import sys', 'import subprocess', 'import builtins',
        'from os ', 'from sys ', 'from subprocess ', 'from builtins ',
        'eval(', 'exec(', 'compile(',
        'open(', 'file(', 'input(',
        'globals(', 'locals(', 'vars(',
        '__code__', '__class__', '__subclasses__',
        'system(', 'popen(', 'spawn', 'getattr(__builtins__',
    ]
    
    def __init__(self):
        pass
    
    def validate_code(self, code: str) -> Tuple[bool, Optional[str]]:
        """
        Validate code for safety before execution.
        
        Returns:
            Tuple of (is_safe, error_message)
        """
        code_lower = code.lower()
        
        # Check for blocked patterns
        for pattern in self.BLOCKED_PATTERNS:
            if pattern.lower() in code_lower:
                return False, f"Blocked pattern detected: {pattern}"
        
        # Try to parse as valid Python
        try:
            ast.parse(code)
        except SyntaxError as e:
            return False, f"Syntax error: {str(e)}"
        
        return True, None
    
    def create_safe_globals(self, df: pd.DataFrame, session: Session) -> Dict[str, Any]:
        """Create a restricted globals dict for code execution."""
        import scipy.stats as scipy_stats
        
        # Build safe builtins: standard safe names + our restricted __import__
        _builtins_src = __builtins__ if isinstance(__builtins__, dict) else __builtins__
        safe_builtins_dict = {}
        for name in self.SAFE_BUILTINS:
            if name == '__import__':
                safe_builtins_dict['__import__'] = _safe_import
                continue
            if isinstance(_builtins_src, dict):
                if name in _builtins_src:
                    safe_builtins_dict[name] = _builtins_src[name]
            else:
                if hasattr(_builtins_src, name):
                    safe_builtins_dict[name] = getattr(_builtins_src, name)
        
        safe_globals = {
            # Core data tools (so code can use pd, np, df without importing)
            'pd': pd,
            'np': np,
            'df': df.copy(),  # Work on a copy
            
            # Statistics
            'stats': scipy_stats,
            
            # Safe builtins (includes restricted __import__ so "import pandas" etc. works)
            '__builtins__': safe_builtins_dict,
            
            # Commonly needed
            'datetime': __import__('datetime'),
            'timedelta': __import__('datetime').timedelta,
            'date': __import__('datetime').date,
        }
        
        # Add pinned definitions as variables
        for name, defn in session.pinned_definitions.items():
            # These will be evaluated in context
            safe_globals[f'_def_{name}'] = defn.formula
        
        return safe_globals
    
    def execute(
        self,
        code: str,
        df: pd.DataFrame,
        session: Session,
        timeout_seconds: int = 30
    ) -> Dict[str, Any]:
        """
        Execute pandas code safely and return the result.
        
        Note: timeout_seconds is reserved for future use (enforcing it would require
        running code in a subprocess). Long-running code can block the worker.
        
        Returns:
            Dict with 'success', 'result', 'result_type', 'stdout', 'error'
        """
        # Validate first
        is_safe, error_msg = self.validate_code(code)
        if not is_safe:
            return {
                "success": False,
                "result": None,
                "result_type": None,
                "stdout": "",
                "error": error_msg
            }
        
        # Prepare execution environment
        safe_globals = self.create_safe_globals(df, session)
        safe_locals = {}
        
        # Capture stdout
        stdout_capture = StringIO()
        stderr_capture = StringIO()
        
        try:
            with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                exec(code, safe_globals, safe_locals)
            
            # Look for result in various places
            result = None
            result_type = None
            
            if 'result' in safe_locals:
                result = safe_locals['result']
            elif 'output' in safe_locals:
                result = safe_locals['output']
            else:
                # Get the last assigned variable
                for var_name in reversed(list(safe_locals.keys())):
                    if not var_name.startswith('_'):
                        result = safe_locals[var_name]
                        break
            
            # Determine result type
            if result is not None:
                if isinstance(result, pd.DataFrame):
                    result_type = "dataframe"
                elif isinstance(result, pd.Series):
                    result_type = "series"
                elif isinstance(result, (int, float, np.integer, np.floating)):
                    result_type = "number"
                elif isinstance(result, str):
                    result_type = "string"
                elif isinstance(result, (list, tuple)):
                    result_type = "list"
                elif isinstance(result, dict):
                    result_type = "dict"
                else:
                    result_type = "other"
            
            return {
                "success": True,
                "result": result,
                "result_type": result_type,
                "stdout": stdout_capture.getvalue(),
                "error": None
            }
            
        except Exception as e:
            return {
                "success": False,
                "result": None,
                "result_type": None,
                "stdout": stdout_capture.getvalue(),
                "error": f"{type(e).__name__}: {str(e)}\n{traceback.format_exc()}"
            }


class QueryExecutor:
    """
    High-level query execution service.
    """
    
    def __init__(self):
        self.safe_executor = SafeExecutor()
    
    async def execute_query(
        self,
        code: str,
        session: Session,
        max_result_rows: int = 1000
    ) -> Dict[str, Any]:
        """
        Execute a query and format the result.
        
        Returns formatted result suitable for API response.
        """
        if not session.active_dataset:
            return {
                "success": False,
                "error": "No active dataset. Please upload a file first.",
                "result": None
            }
        
        df = session.active_df
        
        # Execute code
        exec_result = self.safe_executor.execute(code, df, session)
        
        if not exec_result["success"]:
            return {
                "success": False,
                "error": exec_result["error"],
                "result": None,
                "raw_result": None
            }
        
        # Format result
        result = exec_result["result"]
        formatted = self._format_result(result, max_result_rows)
        
        return {
            "success": True,
            "error": None,
            "result": formatted["data"],
            "raw_result": result,  # Keep the raw DataFrame/result for charting
            "result_type": exec_result["result_type"],
            "display_type": formatted["display_type"],
            "row_count": formatted.get("row_count"),
            "truncated": formatted.get("truncated", False),
            "stdout": exec_result["stdout"]
        }
    
    def _format_result(
        self,
        result: Any,
        max_rows: int
    ) -> Dict[str, Any]:
        """Format execution result for display."""
        
        if result is None:
            return {
                "data": None,
                "display_type": "none"
            }
        
        if isinstance(result, pd.DataFrame):
            truncated = len(result) > max_rows
            df_display = result.head(max_rows)
            
            return {
                "data": {
                    "columns": [str(c) for c in df_display.columns],
                    "data": serialize_df_to_records(df_display),
                    "dtypes": {str(col): str(dtype) for col, dtype in df_display.dtypes.items()}
                },
                "display_type": "table",
                "row_count": len(result),
                "truncated": truncated
            }
        
        if isinstance(result, pd.Series):
            truncated = len(result) > max_rows
            series_display = result.head(max_rows)
            
            return {
                "data": {
                    "name": str(result.name) if result.name else "value",
                    "index": [make_serializable(x) for x in series_display.index],
                    "values": [make_serializable(x) for x in series_display.tolist()],
                    "dtype": str(result.dtype)
                },
                "display_type": "series",
                "row_count": len(result),
                "truncated": truncated
            }
        
        if isinstance(result, (int, float, np.integer, np.floating)):
            return {
                "data": make_serializable(result),
                "display_type": "number"
            }
        
        if isinstance(result, str):
            return {
                "data": result,
                "display_type": "text"
            }
        
        if isinstance(result, dict):
            return {
                "data": self._serialize_dict(result),
                "display_type": "dict"
            }
        
        if isinstance(result, (list, tuple)):
            return {
                "data": [make_serializable(x) for x in list(result)[:max_rows]],
                "display_type": "list",
                "truncated": len(result) > max_rows
            }
        
        # Fallback: convert to string
        return {
            "data": make_serializable(result),
            "display_type": "text"
        }
    
    def _serialize_dict(self, d: dict) -> dict:
        """Recursively serialize dict for JSON."""
        result = {}
        for k, v in d.items():
            key = str(k)
            if isinstance(v, pd.DataFrame):
                result[key] = serialize_df_to_records(v)
            elif isinstance(v, pd.Series):
                result[key] = [make_serializable(x) for x in v.tolist()]
            elif isinstance(v, dict):
                result[key] = self._serialize_dict(v)
            elif isinstance(v, (list, tuple)):
                result[key] = [make_serializable(x) for x in v]
            else:
                result[key] = make_serializable(v)
        return result
    
    @staticmethod
    def _escape_single_quotes(s: str) -> str:
        """Escape single quotes for use inside single-quoted Python string literals."""
        return s.replace("\\", "\\\\").replace("'", "\\'")
    
    async def execute_filter(
        self,
        session: Session,
        filter_expr: str
    ) -> Dict[str, Any]:
        """Apply a filter to the active dataset."""
        escaped = self._escape_single_quotes(filter_expr)
        code = f"result = df.query('{escaped}')"
        return await self.execute_query(code, session)
    
    async def execute_aggregation(
        self,
        session: Session,
        group_by: list,
        agg_column: str,
        agg_func: str
    ) -> Dict[str, Any]:
        """Execute a groupby aggregation."""
        group_cols = ", ".join([f"'{self._escape_single_quotes(c)}'" for c in group_by])
        code = f"result = df.groupby([{group_cols}])['{self._escape_single_quotes(agg_column)}'].{agg_func}().reset_index()"
        return await self.execute_query(code, session)
    
    async def execute_sort(
        self,
        session: Session,
        sort_column: str,
        ascending: bool = True,
        top_n: Optional[int] = None
    ) -> Dict[str, Any]:
        """Sort and optionally limit results."""
        escaped_col = self._escape_single_quotes(sort_column)
        code = f"result = df.sort_values('{escaped_col}', ascending={ascending})"
        if top_n:
            code += f".head({top_n})"
        return await self.execute_query(code, session)


# Singleton instance
query_executor = QueryExecutor()
